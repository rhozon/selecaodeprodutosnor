---
title: "Homework: Econometrics (Shangai University of Finance) "
author: "Ozon, R. H. and Peixoto, F., S."
date: "01/10/2020"
output:
  html_document:
    toc: true
    toc_float: true
---

<!-- ================================================================================= -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#linha para retirar os [1] prefixo de resultados dos output dos comandos do r
# veja https://stackoverflow.com/questions/22524822/how-can-i-remove-the-prefix-index-indicator-1-in-knitr-output/22563357
knitr::opts_chunk$set(opts.label="kill_prefix")
```

```{r include=FALSE}
# cunhk para retiraar os prefixos
default_output_hook <- knitr::knit_hooks$get("output")
# Output hooks handle normal R console output.
knitr::knit_hooks$set( output = function(x, options) {

  comment <- knitr::opts_current$get("comment")
  if( is.na(comment) ) comment <- ""
  can_null <- grepl( paste0( comment, "\\s*\\[\\d?\\]" ),
                     x, perl = TRUE)
  do_null <- isTRUE( knitr::opts_current$get("null_prefix") )
  if( can_null && do_null ) {
    # By default R print output aligns at the right brace.
    align_index <- regexpr( "\\]", x )[1] - 1
    # Two cases: start or newline
    re <- paste0( "^.{", align_index, "}\\]")
    rep <- comment
    x <- gsub( re, rep,  x )
    re <- paste0( "\\\n.{", align_index, "}\\]")
    rep <- paste0( "\n", comment )
    x <- gsub( re, rep,  x )
  }

  default_output_hook( x, options )

})

knitr::opts_template$set("kill_prefix"=list(comment=NA, null_prefix=TRUE))

```

<!-- ================================================================================= -->

***

[See the repo on github](https://github.com/rhozon/homeworkI)

***
2 In the simple linear regression model $y=\beta_{0}+\beta_{1}x+u$, suppose that $E(u)\neq  0$. Letting $\alpha_{0}=E(u)$,
show that the model can always be rewritten with the same slope, but a new intercept and error, where
the new error has a zero expected value.


 **Solution:**

\begin{align}
y=&\beta_{0}+\beta_{1}x+u\quad \mbox{then we can add $\alpha_{0}$ and subtract $\alpha_{0}$}\\
y=&(\alpha_{0}+\beta_{0}) +\beta_{1}x+ (u−\alpha_{0})\\
\end{align}

We can call the new error $e=u-\alpha_{0}$ and so that $E(e) = 0$. The new intercept is $α_0+β_0$, but the slope is still $β_1$.


***

3 The following table contains the ACT scores and the GPA (grade point average) for eight college students. Grade point average is based on a four-point scale and has been rounded to one digit after the decimal



| Student | GPA | ACT |
| ------- | --- | --- |
| 1       | 2.8 | 21  |
| 2       | 3.4 | 24  |
| 3       | 3   | 26  |
| 4       | 3.5 | 27  |
| 5       | 3.6 | 29  |
| 6       | 3   | 25  |
| 7       | 2.7 | 25  |
| 8       | 3.7 | 30  |


(i) Estimate the relationship _GPA_ and _ACT_ using OLS; that is, obtain the intercept and the slope estimates in this equation

$$
\widehat{GPA}=\widehat{\beta}_{0}+\widehat{\beta}_{1}ACT
$$


Comment on the direction of the relationship. Does the intercept have useful interpretation here ? Explain. How much higher is the _GPA_ predicted to be if the _ACT_ score is increased by five points ?

 **Solution**: 

[*Download the table here](https://github.com/rhozon/homeworkI/blob/main/gpaxact.xlsx?raw=true)

The estimated relationship between _GPA_ and _ACT_ is:

$$
\widehat{GPA}=0.568+0.102ACT
$$


In R we can call:




```{r comment=NA, warning=FALSE, message=FALSE}
table<-read.csv(file="https://raw.githubusercontent.com/rhozon/homeworkI/main/gpaxact.csv",head=TRUE,sep=";")

GPA.fitted<-lm(GPA~ACT,data=table)

```

&nbsp;

<center>
```{r chunk 6, echo=FALSE, message=FALSE, results='asis',warning=FALSE}
library(stargazer)
stargazer(GPA.fitted, type="html", 
          intercept.bottom = FALSE, 
          single.row=FALSE,     
          notes.append = FALSE, 
          header=FALSE) 
```

</center>

&nbsp;


The direction of the relationship is positive, i.e. when ACT grows (one point), GPA is expected to grow by about 0.10 score points.  

The intercept shows the GPA value when the other coefficients are equal to zero. That is, if any student in the class gets a score of 0 we expect GPA = 0.568.

If the ACT score is increased by 5 points the GPA will be about 0.51 (=0.102*5) greater.



(ii) Compute the fitted values and residuals for each observation and verify that residuals (approximately) sum to zero.


**Solution**: 

The fitted values are obtained by applying the OLS equation and multiplying by the ACT values. For example, for the second student, we estimate a GPA of 0.568 + 0.102 * 24 = 3.021.

The residuals are obtained by the equation $GPA-\widehat{GPA}$, and the sum $\displaystyle\sum_{i=1}^{n}(GPA-\widehat{GPA})$


```{r echo=FALSE, results='asis', message = FALSE}


table<-read.csv(file="https://raw.githubusercontent.com/rhozon/homeworkI/main/gpaxact.csv",head=TRUE,sep=";")

library(knitr)
library(kableExtra)

kbl(cbind(table)) %>%
  kable_paper() %>%
  scroll_box(width = "600px", height = "200px")%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```



In R we can call:

```{r comment=NA, warning=FALSE, message=FALSE}

sum(GPA.fitted$.residuals)

```



(iii) What is the predicted value of _GPA_ when _ACT_=20 ?

**Solution**:

Using the OLS estimated equation $\widehat{GPA}=0.568+0.102\times 20 = 2.612$


(iv) How much of the variation in _GPA_ for these eight students is explained by _ACT_ ? Explain.

**Solution:** 

By the $R^{2}=0.5774$ we cain expect that about 57.74% of the variations in GPA can be explained by ACT.

***

10 Let $\widehat{\beta}_{0}$ and $\widehat{\beta}_{1}$ be the OLS intercept and slope estimators, respectively, and let $\overline{u}$ be the sample average of errors (not the residuals!).

(i) Show that $\widehat{\beta}_{1}$ can be written as $\widehat{\beta}_{1}=\beta_{1}+\displaystyle\sum_{i=1}^{n}w_{i}u_{i},$ where $w_{i}=d_{i}/\mbox{SST}_{x}$ and $d_{i}=x_{i}-\overline{x}$

 **Solution:** 

If we can call $w=\frac{\displaystyle\sum_{i=1}^{n}(x_{i}-\overline{x})u_{i}}{\displaystyle\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}$ than $\widehat{\beta}_{1}=\beta_{1}+\frac{\displaystyle\sum_{i=1}^{n}(x_{i}-\overline{x})u_{i}}{\displaystyle\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}$.  Then, now, define that $w_{i}=d_{i}/SST_{x}$ .


(ii) Use part (i), along with $\displaystyle\sum_{i=1}^{n}w_{i}u_{i}=0$, to show that $\widehat{\beta}_{1}$ and $\overline{u}$ are uncorrelated. [_Hint:_ You are been asked to show that $E[(\widehat{\beta}_{1}-\beta_{1})\cdot \overline{u}]=0$]  

 **Solution:** 

We show that the latter is zero because $Cov(\widehat{\beta}_{1},\overline{u})=E[(\widehat{\beta}_{1}-\beta_{1})\overline{u}]$.

Remember that $E[(\widehat{\beta}_{1}-\beta_{1})\overline{u}]=E[(\displaystyle\sum^{n}_{i=1}w_{i}u_{i})\overline{u}]=\displaystyle\sum^{n}_{i=1}w_{i}E(u_{i}\overline{u}).$

The $u_{i}$ are pairwise uncorrelated (they are independent), $E(u_{i},\overline{u})=E(u_{i}^{2}/n)=\sigma^{2}/n$ (because $E(u_{i},u_{h})=0,\,\,i\neq h$). Therefore, $\displaystyle\sum_{i=1}^{n}w_{i}E(u_{i},\overline{u})=\sum^{n}_{i=1}w_{i}(\sigma^{2}/n)=(\sigma^{2}/n)\displaystyle\sum^{n}_{i=1}w_{i}=0$ 


(iii) Show that $\widehat{\beta}_{0}$ can be written as $\widehat{\beta}_{0}=\beta_{0}+\overline{u}-(\widehat{\beta}_{1}-\beta_{1})\overline{x}$

 **Solution:**

Remember that the OLS intercept formula is: $\widehat{\beta}_{0}=\overline{y}-\widehat{\beta}\overline{x}$ and if we can insert this in $\overline{y}=\beta_{0}+\beta_{1}\overline{x}+\overline{u}$ we can obtain:

$$
\widehat{\beta}_{0}=(\beta_{0}+\beta_{1}\overline{x}+\overline{u})-\widehat{\beta}_{1}\overline{x}=\beta_{0}+\overline{u}-(\widehat{\beta}-\beta_{1})\overline{x}
$$

(iv) Use parts (ii) and (iii) to show that $Var(\widehat{\beta}_{0}) =\sigma^{2}/n+\sigma^{2}(\overline{x})^{2}/SST_{x}$.


 **Solution:**

$\widehat{\beta}_{1}$ and $\overline{u}$ are uncorrelated than

$$
Var(\beta_{0})=Var(\overline{u})+Var(\widehat{\beta_{1}})\overline{x}^{2}=\\
\sigma^{2}/n+(\sigma^{2}/SST_{x})\overline{x}^{2}=\\
\sigma^{2}/n+\sigma^{2}\overline{x^{2}}/SST_{x}
$$




(v) Do the algebra to simplify the expression in part (iv) to equation (2.58) [_Hint:_ $SST_{x}/n=n^{-1}\displaystyle\sum_{i=1}^{n}x^{2}_{i}-(\overline{x})^{2}$.] 


 **Solution**: 

The substitution gives 


$$
Var(\widehat{\beta}_{0})=\sigma^{2}[SST_{x}/n+\overline{x}^{2}]/SST_{x}=\\
\sigma^{2}[(n^{-1}\sum^{n}_{i=1})x_{i}^{2}-\overline{x}^{2}]/SST_{x}=\\
\sigma^{2}(n^{-1}\sum^{n}_{i=1}x^{2}_{i})/SST_{x}
$$

***

3 The following model is a simplified version of the multiple regression model used by Biddle and Hamermesh (1990) to study the tradeoff between time spent sleeping and working and to look at other factors affecting sleep:


$$
sleep=\beta_{0}+\beta_{1}totwrk+\beta_{2}educ+\beta_{3}age+u,
$$

where $sleep$ and $totwrk$ (total work) are measured in minutes per week and $educ$ and $age$ are measured in years (See also Computer Exercise C3 in Chapter 2.)

(i) If adults trade off sleep for work, what is the sign of $\beta_{1}$ ?

**Solution**:

```{r comment=NA}
library(wooldridge)

str(sleep75)


```


We can run the OLS regression in R

```{r comment=NA}
fit<-lm(sleep~totwrk+educ+age,data=sleep75)

```


&nbsp;

<center>
```{r echo=FALSE, message=FALSE, results='asis'}
#http://people.virginia.edu/~jcf2d/exercises/getting_started_with_stargazer.html
library(stargazer)
stargazer(fit, type="html", 
          column.labels = c("fit"), 
          intercept.bottom = FALSE, 
          single.row=FALSE,     
          notes.append = FALSE, 
          header=FALSE) 
```
</center>

&nbsp;


```{r}
#generating the dataset
library(xlsx)
write.xlsx(sleep75, file="sleep75.xlsx")
```

[You can download the dataset here](https://github.com/rhozon/homeworkI/blob/main/sleep75.xlsx?raw=true)


Soon if adults trade off sleep for work, more work implies less sleep (other things equal), so $β_1 < 0$.

(ii) What signs do you think $\beta_{2}$ and $\beta_{3}$ will have ?


**Solution** 


The signs of $\beta_2$ and $\beta_3$ are not obvious. One could argue that more educated people like to get more out of life, and so, other things equal, they sleep less ($\beta_2 < 0$). The relationship between sleeping and age is more complicated than this model suggests, and economists are not in the best position to judge such things.


(iii) Using th data in SLEEP75, the estimated equation is 

$$
\widehat{sleep}=3,638.25-.148totwrk-11.13educ+2.20age\\
n=706,\,\, R^{2}=.113.
$$


If someone works five more hours per week, by how many minutes is _sleep_ predicted to fall ? Is this a large tradeoff ?

**Solution** 

Since $totwrk$ is in minutes, we must convert five hours into minutes: $∆totwrk = 5(60) = 300$. Then sleep is predicted to fall by 0.148(300) = 44.4 minutes. Fo a week, 45 minutes less sleep is not an overwhelming change

(iv) Discuss the sign and magnitude of the estimated coefficient on _educ_

**Solution** 

More education implies less predicted time sleeping, but the effect is quite small.
If we assume the difference between college and high school is four years, the
college graduate sleeps about 45 minutes less per week, other things equal



(v) Would you say $totwrk$, $educ$ and $age$ explain much of the variation in sleep? What other factors might affect the time spent sleeping ? Are these likely to be correlated with $totwrk$ ?

**Solution** 

As shown in the (i) the $R^2=0.113$ that the three explanatory variables explain only about 11.3% of the variation in sleep. One important factor in the error term is general health.

Another is marital status, and whether the person has children. Health (however we measure that), marital status, and number and ages of children would generally be correlated with totwrk. (For example, less healthy people would tend to work less.)


***

9 The following equation describes the median housing price in a community in terms of amount of pollution (nox for nitrous oxide) and the average number of rooms in houses in the community (rooms):

$$
\log(price)=\beta_{0} +\beta_{1}\log(nox)+\beta_{2}rooms+u.
$$

(i) What is the interpretation of $\beta_{1}$ and $\beta_{2}$? What is the interpretation of $\beta_{1}$ ? Explain.

**Solution** 

We hope $\beta_1 < 0$ because more pollution can be expected to lower housing values; $\beta_1$ is the elasticity of price with respect to $nox$. $\beta_2$ is probably positive because $rooms$ roughly measures the size of a house. (However, it does not allow us to distinguish homes where each room is large from homes where each room is small.)


(ii) Why might $nox$ [or more precisely $\log(nox)$] and $rooms$ be negatively correlated? If this is the case, does the simple regression of $log(price)$ on $log(nox)$ produce an upward or a downward biased estimator of $\beta_{1}$?

**Soloution**


If we assume that $rooms$ increases with quality of the home, then $log(nox)$ and $rooms$ are negatively correlated when poorer neighborhoods have more pollution, something that is often true. If $\beta_2 > 0$ and $Corr(x1, x2) < 0$, the simple regression estimator $\widetilde{β}_1$ has a downward bias. But because $\beta_1 < 0$, this means that the simple regression, on average, overstates the importance of pollution. $(E(\widetilde{\beta}_1)$ is more negative than $\beta_1$.)
 
(iii) Using the data in HPRICE2, the following equations were estimated:

$$
\widehat{\log(price)}=11.71-1.043\log(nox), n=506,\,R^{2}=.264\\
\widehat{\log(price)}=9.23-.718\log(nox)+.306rooms, n=506,\,R^{2}=.514
$$


Is the relationship between the simple and multiple regression estimates of the elasticity of $price$ with respect to $nox$ what would have predicted, given your answer in part (ii)? Does this mean that -.718 is definitely closer to the true elasticity than -1.043 ? 

**Solution:**


This is what we expect from the typical sample based on our analysis in part (i). The simple regression estimate, −1.043, is more negative (larger in magnitude) than the multiple regression estimate, −0.718. As those estimates are only for one sample, we can never know which is closer to $\beta_1$. But if this is a “typical” sample, $\beta_1$ is closer to −0.718.




***


16 The following equations were estimated using the data in LAWSCH85:

\begin{align}
\widehat{lsalary}=& 9.9 -.0041rank +.294GPA\\
&(.24)\quad(.0003)\quad(.069)\\
&n=142\quad R^{2}=.8238
\end{align}


\begin{align}
\widehat{lsalary}=& 9.86 -.0038rank +.295GPA+.00017age\\
&(.29)\quad(.0004)\,\,\,\,\quad(.083)\quad\,\,(.00036)\\
&n=99\quad R^{2}=.8036
\end{align}


How can it be that the $R^{2}$ is smaller when the variable $age$ is added to equation.

**Solution**:


By using the wooldridge package in R we can see:

```{r comment=NA, message=FALSE}
library(wooldridge)

#inspect the dataset
str(lawsch85)


head(lawsch85)

sum(is.na(lawsch85))

na_count <-sapply(lawsch85, function(y) sum(length(which(is.na(y)))))

na_count <- data.frame(na_count)

na_count

sum(na_count)

  
#generating the dataset
library(xlsx)
write.xlsx(lawsch85, file="lawsch85.xlsx")
```


[Now you can download the lawsch85.xlsx here](https://github.com/rhozon/homeworkI/raw/main/lawsch85.xlsx)


By running the two regression models, we can have:

```{r message=FALSE, comment=NA}

mod1<-lm(lsalary~rank+GPA,data=lawsch85)

mod2<-lm(lsalary~rank+GPA+age,data=lawsch85)

```

&nbsp;

<center>
```{r echo=FALSE, message=FALSE, results='asis'}
#http://people.virginia.edu/~jcf2d/exercises/getting_started_with_stargazer.html
library(stargazer)
stargazer(mod1, mod2, type="html", 
          column.labels = c("mod1", "mod2"), 
          intercept.bottom = FALSE, 
          single.row=FALSE,     
          notes.append = FALSE, 
          header=FALSE) 
```
</center>

&nbsp;

The coefficient of the age variable was shown to be insignificant in model 2.


With different observations the $R^2$ are been incomparable (see the $n$ in the two estimated equations). The residual standard error consumed more degrees of freedom in model 2 and increased compared to model 1.As seen above, the age variable has 45 missing data and the other variables in the both models...

```{r comment=NA, message=FALSE}
library(dplyr)
regvars<-lawsch85%>%
select(lsalary,rank,GPA,age)

na_count_reg <-sapply(regvars, function(y) sum(length(which(is.na(y)))))

na_count_reg <- data.frame(na_count_reg)

na_count_reg
```



Although the general $F$ test is significant in both models, the $t$ statistic shows that the inclusion of the $age$ variable does not improve the model's fit to explain lsalary. 




***

### References


[CRAN Wooldridge documentation: Data Sets from "Introductory Econometrics: A Modern
Approach, 6e" by Jeffrey M. Wooldridge](https://cran.r-project.org/web/packages/wooldridge/wooldridge.pdf)



[Shangai University of Finance: Avaible in http://english.sufe.edu.cn/](http://english.sufe.edu.cn/)



[Wooldridge, J. _**Introductory econometrics: A modern approach**_, 2016, 6.ed., Cengage Learning, ISBN 13:
978-1-305-27010-7 ](https://book4you.org/book/3372803/9610fc)













