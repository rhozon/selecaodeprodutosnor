---
title: "Aplicação de GARCH Bayesiano em séries de retornos de preços de commodities"
author: "Rodrigo Hermont Ozon"
date: "Last Update: `r format(Sys.time(), '%B %d, %Y %H:%M:%S')`"
output: 
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
---

```{r starttime}

start_time <- Sys.time() # Execution timing document counter

```


```{css toc-content, echo = FALSE}

#TOC {
  left: 220px;
  margin: 50px 30px 55px 30px;
}

.main-container {
    margin-left: 300px;
}

```


```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	comment = NA
)
knitr::opts_chunk$set(comment = NA)    # Remove all coments # of R outputs
knitr::opts_chunk$set(warning = FALSE) # Remove all warnings # of R outputs
knitr::opts_chunk$set(message = FALSE) # Remove all messages # of R outputs

```

***

<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

</style>

<div class="alert alert-info">

  <strong> Bayesian Estimation of the GARCH(1,1) Model with Student-t Innovations </strong> 
  
</div>

***


<center>

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="comment">

Reprodução do modelo original do artigo de Ardia e Hoogerheide (2010). 

Neste post apresentaremos o uso do pacote bayesGARCH que fornece funções para a estimação Bayesiana do modelo GARCH(1,1) parcimonioso e efetivo com distribuição assimétrica de erros Student-t. O procedimento de estimativa é totalmente automático e, portanto, evita a tediosa tarefa de tunelamento de um algoritmo de amostragem MCMC. O uso do pacote é mostrado em uma aplicação empírica para log-retornos da commoditie milho e soja.


</p>

</center>

***


# Introdução

Pesquisas sobre a mudança de volatilidade usando séries temporais modelos estão presentes desde o artigo pioneiro de Engle (1982). A partir daí, ARCH (modelos autoregressivos de heterocedasticidade condicional) e GARCH (ARCH Generalizado) cresceram rapidamente em uma rica
família de modelos empíricos para previsão de volatilidade durante os anos 80. Esses modelos são difundidos e ferramentas essenciais em econometria financeira.

No modelo GARCH($p, q$) introduzido por Bollerslev (1986), a variância condicional no tempo $t$ do
retorno logarítmico de $y_t$ (de um ativo financeiro ou índice financeiro), denotado por $h_t$, é postulado como uma função linear dos quadrados de $q$ passados log-retornos e variâncias condicionais passadas $p$. Mais precisamente:

$$
h_t = \alpha_0 + \sum_{i=1}^{q}\alpha_i y^{2}_{t-1} + \sum_{j=1}^{p}\beta_j h_{t-j},
$$


onde os parâmetros satisfazem as restrições: $\alpha_i \geq 0\,\,(i=0,\cdots,q)$ e $\beta_j \geq 0\,\,(j=1,\cdots,p)$ para garantir uma variância condicional positiva. Na maioria das aplicações empíricas verifica-se que a simples especificação $p = q = 1$ é capaz de reproduzir a dinâmica da volatilidade dos dados financeiros. Isso tem levado a Modelo GARCH(1,1) para se tornar o modelo de trabalho (_workhorese model_) por acadêmicos e profissionais. Dado um modelo especificação para $h_t$, os log-retornos são então modelados como $y+t = \epsilon_t h_{t}^{1/2}$, onde $\epsilon_t$ são perturbações i.d.d. Escolhas comuns para $\epsilon_t$ são distribuições do tipo Normal ou Student-$t$. A distribuição Student-$t$ é particularmente útil uma vez que pode fornecer o excesso de curtose na distribuição condicional que é frequentemente encontrada em processos de séries temporais financeiras (ao contrário de modelos com distribuições normais).

Até recentemente, os modelos GARCH eram principalmente estimados usando a técnica clássica da máxima verossimilhança. Vários pacotes do R fornecem funções para sua estimativa; ver, por exemplo ``fGarch`` (Wuertz e Chalabi, 2009), ``rgarch`` (Ghalanos, 2010) e ``tseries`` (Trapletti and Hornik, 2009). A abordagem Bayesiana oferece uma alternativa atraente que permite obter resultados em pequenas amostras, estimativa robusta, discriminação de modelo, combinação de modelo e declarações probabilísticas em funções (possivelmente não lineares) dos parâmetros do modelo.

O pacote ``bayesGARCH`` (Ardia, 2007) implementa o procedimento de estimação Bayesiana descrita
em Ardia (2008, capítulo 5) para o modelo GARCH(1,1) com distribuição de Student$-t$. A abordagem, baseada sobre o trabalho de Nakatsuma (1998), consiste em uma Algoritmo Metropolis-Hastings (MH) onde as distribuições propostas são construídas a partir de processos ARMA nas observações ao quadrado. Esse A metodologia evita a tarefa demorada e difícil, especialmente para não especialistas, de escolher e ajustar um algoritmo de amostragem. O programa é escrito em R com algumas sub-rotinas implementadas em C para acelerar o procedimento de simulação. A validade do algoritmo, bem como a exatidão do código de computador foi verificado pelo método de Geweke (2004).

# Modelagem, _prioris_ e esquema MCMC

Um modelo GARCH(1,1) com distribuição assimétrica Student$-t$ para os log-returnos $\{y_t\}$ podem ser escritos via aumento de dados (ver Geweke, 1993) como

$$
y_t = \epsilon_t (\frac{v-2}{v}\omega_t h_t )^{1/2}\quad t = 1,\cdots,T
$$
$$
\epsilon_t \sim idd\,\,N(0,1)
$$

$$
\omega_t \sim idd\,\,IG(\frac{v}{2},\frac{v}{2})
$$

$$
h_t = \alpha_0 +\alpha_1 y^{2}_{t-1}+\beta h_{t-1},
$$
onde $\alpha_0 >0, \alpha_1,\beta_1\geq 0$ e $v>2$ seguem $N(0,1)$ denota a distribuição normal padrão.$IG$ a gama inversa. As restrições nos graus de liberdade dos parâmetros $v$ garante que a variância condicional seja finita e as restrições nos parâmetros do modelo GARCH $\alpha_0,\alpha_1$ e $\beta$ garantem sua positividade. Ressaltamos o fato de que apenas restrições de positividade são implementadas no algoritmo Metropolis-Hastings; não são impostas condições de estacionaridade no procedimento de simulação.


Para escrevermos a função de verossimilhança, definimos o vetor $y = (y_1,\cdots,y_T)', \omega = (\omega_1,\cdots,\omega_T)'$ e $\alpha = (\alpha_0,\alpha_1)'$. Reagrupamos os parâmetros do modelo no vetor $\psi = (\alpha,\beta,v)$. Em seguida, ao definir a matriz diagonal $T\times T$

$$
\sum = \sum(\psi,\omega)=diag(\{\omega_t \frac{v-2}{v}h_t(\alpha,\beta)\}^{T}_{t=1})
$$

onde $h_t(\alpha,\beta)= \alpha_0+\alpha_1y^{2}_{t-1}+\beta h_{t-1}(\alpha,\beta)$ podemos expressar a verossimilhança de ($\psi,\omega$) como:

$$
L(\psi, \omega|y) \propto (det\sum)^{-1/2}exp[-\frac{1}{2}y'\sum^{-1}y]
$$
A abordagem bayesiana considera ($\psi, \omega$) como variáveis aleatórias que são caracterizadas pelas densidades _à priori_ denotada por $p(\psi,\omega)$. A _prioiri_ é especificada com a ajuda
de parâmetros chamados hiperparâmetros que são inicialmente considerados conhecidos e constantes. Além disso, dependendo da informação prévia do pesquisador, esta densidade pode ser mais ou menos informativa. Então, se acoplando a função de verossimilhança dos parâmetros do modelo com a densidade anterior, podemos transformar a densidade de probabilidade usando a regra de Bayes para obter a densidade _à posteriori_ $p(\psi,\omega|y)$ como:

$$
p(\psi, \omega|y) = \frac{L(\psi,\omega|y)p(\psi,\omega)}{\int L(\psi,\omega|y)p(\psi, \omega)d\psi d \omega}
$$
Esta _posteriori_ é uma descrição quantitativa e probabilística do conhecimento sobre os parâmetros do modelo após observar os dados. Para uma excelente introdução à econometria bayesiana, remetemos o leitor para Koop (2003).


Usamos _prioris_ truncadas normais nos parâmetros GARCH $\alpha$ e $\beta$

$$
p(\alpha)\propto\phi_{N_2}(\alpha|\mu_\alpha,\Sigma_\alpha) 1 \{\alpha\in R_+^2\}
$$
$$
p(\beta)\propto\phi_{N_1}(\beta|\mu_\beta,\Sigma_\beta) 1 \{\beta\in R_+\}
$$
onde:

$\mu_\bullet$ e $\sum_\bullet$ são os hiperparâmetros; 

$1\{\bullet\}$ é a função indicadora;

$\phi_{N_d}$ é a densidade normal $n-$dimensional;

A distribuição _à priori_ do vetor condicional $\lambda=(\lambda_1,...\lambda_T)'$ em $v$ é encontrado observando que os componentes $\lambda_t$ são independentes e identicamente distribuídos da gama invertida, que produz:

$$
p(\lambda|v)=\left(\frac{v}{2} \right)^{\frac{Tv}{2}}\left[ \Gamma\left(\frac{v}{2}\right)\right]^{-T}\left(\prod_{t=1}^T\lambda_t\right)^{-\frac{v}{2}-1}exp\left[-\frac{1}{2}\sum_{t=1}^T\frac{v}{\lambda_t}\right]
$$

além disso, a distribuição a priori nos parâmetros dos graus de liberdade é uma exponencial traduzida com parâmetros $\lambda^*>0$ e $\delta\geq2$.

$$
p(v)=\lambda^*exp[-\lambda^*(v-\delta)]1\{v>\delta\}
$$
Seguimos Deschamps (2006) na escolha prévia na distribuição dos parâmetros de graus de liberdade. A distribuição é uma exponencial traduzida com parâmetros $\lambda > 0$ e $\delta \geq 2$

$$
p(v)=\lambda^*exp[-\lambda^*(v-\delta)]1\{v>\delta\}
$$

Para grandes valores de $\lambda$, a distribuição de massa da _priori_ é concentrada na vizinhança de $\delta$ e uma restrição sobre os graus de liberdade pode ser imposta nesta forma. A normalidade dos erros é assumida quando $\delta$ é escolhido grande. Como aponta Deschamps (2006), essa densidade prévia é útil por dois motivos. Primeiro, é potencialmente importante, por razões numéricas, limitar o parâmetro de graus de liberdade longe de dois para evitar a explosão da variância condicional. Em segundo lugar, podemos aproximar a normalidade dos erros enquanto mantemos uma _priori_ que pode melhorar a convergência do amostrador.

A distribuição _a priori_ conjunta é então formada assumindo a independência _a priori_ entre os parâmetros $p(\psi,\omega)=p(\alpha)p(\beta)p(\omega|v)p(v)$. A natureza recursiva da variância na equação GARCH(1,1) implica que a _à posteriori_ conjunta e as densidades condicionais não podem ser expressas neste formato. Não existe nenhuma _priori_ (conjugada) que possa remediar esta propriedade. Portanto, não podemos usar o amostrador de Gibbs simples e precisamos contar com uma estratégia de simulação de Monte Carlo Markov Chain (MCMC) mais elaborada para aproximar a densidade _à posteriori_.

A ideia de amostragem MCMC foi introduzida pela primeira vez por Metropolis et al. (1953) e foi posteriormente generalizado por Hastings (1970). A estratégia de amostragem baseia-se na construção de uma cadeia de Markov com realizações $(\psi^{[0]},\omega^{[0]}),\cdots,(\psi^{[j]},\omega^{[j]}),\cdots,$ no espaço de parâmetros. Em condições de regularidade apropriadas, resultados assintóticos garantem que como $j$ tende a infinito ($\psi^{[j]},\omega^{[j]}$) tende em distribuição para uma variável aleatória cuja densidade é $p(\psi,v | y)$. Assim, após descartar um _burn-in_ dos primeiros sorteios, os valores realizados da cadeia pode ser usada para fazer inferência sobre a _posteriori_ conjunta.


O amostrador MCMC implementado no pacote ``bayesGARCH`` é baseado na abordagem de Ardia
(2008, capítulo 5), inspirado no trabalho anterior de Nakatsuma (1998). O algoritmo consiste em um algoritmo Metropolis-Hastings onde os parâmetros GARCH são atualizados por blocos (um bloco para $\alpha$ e um bloco para $\beta$) enquanto o parâmetro de graus de liberdade é amostrado usando uma técnica de rejeição otimizada de uma densidade de fonte exponencial traduzida. Esta metodologia tem a vantagem de ser totalmente automática. Além disso, em nossa experiência, o algoritmo explora a domínio da _posteriori_ conjunta de forma eficiente em comparação com abordagens _naive_ de Metropolis-Hastings ou do amostrador Griddy-Gibb de Ritter and Tanner (1992).

# Aplicação em retornos de preços de _commodities_

Utilizaremos o método de estimação bayesiana para dados diários de dois ativos negociados na CBOT, a saber: (https://finance.yahoo.com/commodities/)

- Milho (em US\$/bushel) à futuro (ticker ``ZC=F``)

- Soja (em US\$/bushel) à futuro (ticker ``ZS=F``)

Utilizamos as séries temporais com início em 01-01-2010 até a data mais recente, ou seja, o último dia de negociação disponível (comando ``Sys.Date()`` = `r Sys.Date()`)

```{r fig.width=9, fig.height=3}

library(quantmod)

Milho <- getSymbols("ZC=F", auto.assign = FALSE,
                    from = "2010-01-01", end = Sys.Date())

Soja  <- getSymbols("ZS=F", auto.assign = FALSE,
                    from = "2010-01-01", end = Sys.Date()) 

library(fpp3)
library(ggplot2)

fech_Milho <- autoplot(Cl(Milho)) + xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 9, face = "bold")) +
  ylab("Fechamento (CORN futures)")

ret_Milho <- autoplot(diff(log(Cl(Milho)))) + xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 9, face = "bold")) +
  ylab("log retornos do fechamento (CORN futures)")

hist_Milho <- ggplot(Milho, aes(x = diff(log(Cl(Milho))) )) + 
 geom_histogram(aes(y = ..density..), colour = "black", fill = "white")+
 geom_density(alpha = .2, fill = "#FF6666") + xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("log retornos (CORN futures)")
  
fech_Soja <- autoplot(Cl(Soja)) + xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 9, face = "bold")) +
  ylab("Fechamento (Soybean futures)")

ret_Soja <- autoplot(diff(log(Cl(Soja)))) + xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 9, face = "bold")) +
  ylab("log retornos do fechamento (Soybean futures)")

hist_Soja <- ggplot(Soja, aes(x = diff(log(Cl(Soja))) )) + 
 geom_histogram(aes(y = ..density..), colour = "black", fill = "white")+
 geom_density(alpha = .2, fill = "#FF6666") + xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("log retornos (Soybean futures)")

library(patchwork)

fech_Milho + ret_Milho + hist_Milho

fech_Soja + ret_Soja + hist_Soja

```


Criamos primeiramente dois _dataframes_ somente com os vetores de cada _commoditie_:

```{r}

retornos_Milho <- diff(log(Cl(Milho))) %>%
  as.data.frame() %>%
  drop_na() %>%
  rename("log_ret_milho" = `ZC=F.Close`) %>% 
  glimpse()

retornos_Milho <- retornos_Milho[1:nrow(retornos_Milho),] 
retornos_Milho <- 100*as.vector(retornos_Milho)

retornos_Soja <- diff(log(Cl(Soja))) %>%
  as.data.frame() %>%
  drop_na() %>%
  rename("log_ret_soja" = `ZS=F.Close`) %>% 
  glimpse()

retornos_Soja<- retornos_Soja[1:nrow(retornos_Soja),] 
retornos_Soja <- 100*as.vector(retornos_Soja)

```

Então rodamos o modelo GARCH(1,1) bayesiano via Monte Carlo Markov Chain. Como distribuição _à priori_ para a estimativa Bayesiana, tomamos os valores padrão do pacote ``bayesGARCH``, que
são _prioris_ difusas. Geramos duas cadeias para 20000 passos cada uma definindo os valores dos parâmetros de controle ``n.chain = 2`` e ``l.chain = 20000``.

```{r}

library(bayesGARCH)

set.seed(1234)

MCMC_Milho <- bayesGARCH(retornos_Milho, control = list(l.chain = 20000, n.chain = 2))

MCMC_Soja <- bayesGARCH(retornos_Soja, control = list(l.chain = 20000, n.chain = 2))

```

A função gera as cadeias de Markov via Monte Carlo como um objeto da classe ``mcmc`` do pacote ``coda`` (Plummer _et al._, 2010). Este pacote contém funções para pós-processamento da saída MCMC; veja Plummer _et alli_ (2006) para uma introdução. Observe que o ``coda`` ainda não é carregado automaticamente com ``bayesGARCH``.

Um gráfico de convergência das Cadeias de Markov via Monte Carlo (MCMC) (ou seja, um gráfico de iterações vs. valores amostrados) podem ser gerados usando a função ``traceplot``; a saída é exibida nos gráficos a seguir:

```{r fig.width=9, fig.height=11}

plot(MCMC_Milho)

```

e da mesma maneira temos o Monte Carlo Markov Chain para os log-retornos dos preços da soja:

```{r fig.width=9, fig.height=11}

plot(MCMC_Soja)

```

A convergência do amostrador (usando o diagnóstico teste de Gelman e Rubin (1992)), taxas de aceitação e autocorrelações nas cadeias podem ser computadas do seguinte modo:

```{r fig.width=9, fig.height=11}

library(coda)

gelman.diag(MCMC_Milho)

1 - rejectionRate(MCMC_Milho)

autocorr.diag(MCMC_Milho)

```

E para a soja temos:

```{r fig.width=9, fig.height=11}

gelman.diag(MCMC_Soja)

1 - rejectionRate(MCMC_Soja)

autocorr.diag(MCMC_Soja)

```


O diagnóstico de convergência não mostra evidências para a estabilização até as 3000 primeiras iterações (apenas a segunda metade da cadeia é usada por padrão em ``gelman.diag``) já que o fator de redução de escala é menor que 1.01 e 1 para ambas as séries de retornos; (ver Gelman e Rubin (1992) para detalhes). O algoritmo de amostragem MCMC atinge taxas muito altas de aceitação variando de 93% para o vetor $\alpha$ a 97% para $\beta$ sugerindo que as distribuições propostas estão próximas das condicionais completas. A técnica de rejeição usada para gerar $ν$ permite que um novo valor seja desenhado em cada passo no algoritmo Metropolis-Hastings.

As autocorrelações de um _lag_ no _range_ das cadeias vão de 0,87 para o parâmetro $\alpha_1$ a 0,96 para o parâmetro $v$ no milho e de 0.9 a 0.97 na soja. Usando a função ``formSmpl``, descartamos as primeiras 10000 extrações da saída geral do MCMC como um _burn in_ no período, mantendo apenas a cada segundo sorteio para diminuir a autocorrelação e mesclar as duas cadeias para obter um comprimento de amostra final de 10000.

```{r}

smpl_Milho <- formSmpl(MCMC_Milho, l.bi = 10000, batch.size = 2)

smpl_Soja <- formSmpl(MCMC_Soja, l.bi = 10000, batch.size = 2)

```

Estatísticas básicas da _posteriori_ podem ser facilmente obtidas com o método/comando de ``summary`` disponível para objetos ``mcmc``.

```{r}

summary(smpl_Milho)

```

E o output do modelo GARCH bayesiano para soja, fica:

```{r}

summary(smpl_Soja)

```

As _posterioris_ marginais são demonstradas nos gráficos para cada parâmetro do modelo:

```{r fig.width=9, fig.height=3}

df_smpl_Milho <- as.data.frame(smpl_Milho) %>% 
  mutate(alpha1_plus_beta = alpha1+beta)

df_smpl_Soja <- as.data.frame(smpl_Soja) %>% 
  mutate(alpha1_plus_beta = alpha1+beta)

a<-
ggplot(df_smpl_Milho, aes(x=alpha0 )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("alpha0 Milho")

b<-
ggplot(df_smpl_Milho, aes(x=alpha1 )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("alpha1 Milho")

c<-
ggplot(df_smpl_Milho, aes(x=beta )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("beta Milho")

e<-
ggplot(df_smpl_Milho, aes(x=nu )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666") + xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("nu Milho")

a+b

c+e

```

E para a soja:

```{r fig.width=9, fig.height=3}
a<-
ggplot(df_smpl_Soja, aes(x=alpha0 )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("alpha0 Soja")

b<-
ggplot(df_smpl_Soja, aes(x=alpha1 )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("alpha1 Soja")

c<-
ggplot(df_smpl_Soja, aes(x=beta )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("beta Soja")

e<-
ggplot(df_smpl_Soja, aes(x=nu )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666") + xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("nu Soja")

a+b

c+e

```


Notamos claramente a forma assimétrica dos histogramas; em especial para o parâmetro $ν$.

Isso também se reflete nas diferenças entre as médias da _posterioris_ e medianas. Esses resultados devem nos alertar contra o uso abusivo de justificativas assintóticas. No presente caso, mesmo um pouco mais de 3000 observações não são suficientes para justificar a simetria assintótica de uma aproximação normal para o estimador de parâmetros da distribuição.

Declarações probabilísticas sobre funções não lineares dos parâmetros do modelo podem ser obtidas diretamente por simulação da amostra _posteriori_ conjunta. Em particular, podemos testar a condição de estacionaridade da covariância e estimar a densidade da variância incondicional quando esta condição for satisfeita. Sob a especificação GARCH(1,1), o processo é de covariância estacionária se $\alpha_1 + \beta < 1$, como mostrado por Bollerslev (1986, página 310). O termo ($\alpha_1$ + $\beta$) é o grau de persistência na autocorrelação ao quadrado que controla a intensidade do agrupamento no processo de volatilidade. Com um valor próximo de um, choques e variações passadas terão um impacto mais longo sobre a variância condicional futura.

Para fazer inferência sobre a persistência ao quadrado, simplesmente usamos a amostra _à posteriori_ e geramos ($\alpha^{[j]}_{1} + \beta^{[j]}$) para cada sorteio $\psi^{[j]}$ dentro a amostra _à posteriori_. A densidade da persistência da _posteriori_ é plotada nos gráficos a seguir para ambas as séries de preços das _commodities_:

```{r fig.width=9, fig.height=4}

amaisb_milho <-
ggplot(df_smpl_Milho, aes(x=alpha1_plus_beta )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+
 geom_vline(aes(xintercept = 1))+ xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("alpha1_plus_beta Milho") +
  ggtitle("Densidade da persistência na posteriori conjunta com base nas 10000 simulações")

amaisb_soja <-
ggplot(df_smpl_Soja, aes(x=alpha1_plus_beta )) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+
 geom_vline(aes(xintercept = 1))+ xlab("") +
  theme(axis.title.y = element_text(size = 7, angle = 90)) +
  theme(plot.title = element_text(size = 7, face = "bold")) +
  ylab("alpha1_plus_beta Soja")

amaisb_milho + amaisb_soja

```

A assimetria de ambos os histogramas é calculada a seguir, lembrando que:

- Assimetria negativa (á esquerda) = $Moda>Mediana>Media$

- Assimetria positiva (á direita) = $Moda<Mediana<Media$

Calculamos que a assimetria da distribuição da soma dos parâmetros $\alpha_1$ e $\beta$ do milho são assimétricos à esquerda e a da soja à direita.

```{r}

Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

moda_milho <- Mode(df_smpl_Milho$alpha1_plus_beta)
moda_milho

mediana_milho <- median(df_smpl_Milho$alpha1_plus_beta)
mediana_milho

media_milho <- mean(df_smpl_Milho$alpha1_plus_beta)
media_milho

#max_milho <- max(df_smpl_Milho$alpha1_plus_beta)
#max_milho

ifelse(moda_milho>mediana_milho & mediana_milho>media_milho, "Assimétrica á esquerda", "Assimétrica à direita")

```

E no caso da soma de $\alpha_1 + \beta$ para a soja, temos:

```{r}

moda_soja <- Mode(df_smpl_Soja$alpha1_plus_beta)
moda_soja

mediana_soja <- median(df_smpl_Soja$alpha1_plus_beta)  
mediana_soja

media_soja <- mean(df_smpl_Soja$alpha1_plus_beta)
media_soja

#max_soja <- max(df_smpl_Soja$alpha1_plus_beta)  
#max_soja

ifelse(moda_soja > mediana_soja & mediana_soja > media_soja, "Assimétrica á esquerda", "Assimétrica à direita")

```


Neste caso, a estacionaridade da covariância do processo é suportada pelos dados. A variância incondicional do modelo GARCH(1,1) é $\alpha_0 / (1-\alpha_1 - \beta)$ dado que $\alpha_1 + \beta < 1$. 

```{r }

df_smpl_Milho <- df_smpl_Milho %>%
  mutate(
    var.inc = 
      ifelse(
        alpha1+beta <1,
        alpha0 / (1 - alpha1 - beta),
        NA
        )
  )

df_smpl_Soja <- df_smpl_Soja %>%
  mutate(
    var.inc = 
      ifelse(
        alpha1+beta <1,
        alpha0 / (1 - alpha1 - beta),
        NA
        )
  )

```


As médias dessas variâncias incondicionais para cada uma das _commodities_ será de:

```{r}

mean(df_smpl_Milho$var.inc, na.rm = TRUE) # Variância Incondicional parametros milho

mean(df_smpl_Soja$var.inc, na.rm = TRUE) # Variância Incondicional parametros soja

```

Podemos construir um intervalo confiável de 90% para essa médias de variâncias incondicionais:

```{r}

ci_90_Milho <- quantile(df_smpl_Milho$var.inc, probs = c(0.05, 0.95), na.rm = TRUE)
ci_90_Milho

ci_90_Soja <- quantile(df_smpl_Soja$var.inc, probs = c(0.05, 0.95), na.rm = TRUE)
ci_90_Soja

```


Outras declarações probabilísticas sobre interessantes funções dos parâmetros do modelo podem ser obtidas usando a amostra _à posteriori_ conjunta. De acordo com a especificação (1, início da descrição do modelo GARCH Bayesiano na metodologia), a curtose condicional é $3(ν − 2)/(ν − 4)$ desde que $ν > 4$. Usando a amostra _à posteriori_, temos como estimar a probabilidade _à posteriori_ de existência para a curtose condicional para cada uma das _commodities_

```{r}

df_smpl_Milho <- df_smpl_Milho %>%
  mutate(
    curt.inc = 
      ifelse(
        nu > 4,
        3*(nu-2) / (nu -4) ,
        NA
        )
  )

mean(df_smpl_Milho$curt.inc, na.rm = TRUE) # Curtose condicional nu milho

median(df_smpl_Milho$curt.inc, na.rm = TRUE) # Mediana nu milho

var(df_smpl_Milho$curt.inc, na.rm = TRUE) # Variância empírica nu milho

```

E para a soja temos:


```{r}

df_smpl_Soja <- df_smpl_Soja %>%
  mutate(
    curt.inc = 
      ifelse(
        nu > 4,
        3*(nu-2) / (nu -4) ,
        NA
        )
  )

mean(df_smpl_Soja$curt.inc, na.rm = TRUE) # Curtose condicional nu Soja

median(df_smpl_Soja$curt.inc, na.rm = TRUE) # Mediana nu Soja

var(df_smpl_Milho$curt.inc, na.rm = TRUE) # Variância empírica nu Soja

```

Condicionalmente após a existência, a média _à posteriori_ da curtose é 16 (milho), 9.02 (soja) a mediana é 9.21 (milho) e 8.07 (soja) e o intervalo confiável de 95% é dado a seguir, indicando caudas mais pesadas do que para uma distribuição normal.

```{r}

ci_95_Milho <- quantile(df_smpl_Milho$curt.inc, probs = c(0.025, 0.975), na.rm = TRUE)
ci_95_Milho

ci_95_Soja <- quantile(df_smpl_Soja$curt.inc, probs = c(0.025, 0.975), na.rm = TRUE)
ci_95_Soja

```


A assimetria positiva da _posteriori_ para a curtose condicional é causada por alguns valores muito grandes (o valor máximo simulado é `r max(df_smpl_Milho$curt.inc)` (milho) e `r max(df_smpl_Soja$curt.inc)` (soja)). Estes valores correspondem a empates com $ν$ ligeiramente maior que 4.

Note que se algum pesquisador desejar lidar com grandes valores para a curtose condicional de antemão, então pode-se definir $\delta > 4$ na _priori_ para $ν$. Por exemplo, a escolha $\delta = 4,5$ garantiria que a curtose fosse menor que 15.

# Restições na _priori_ e perturbações normais

O parâmetro de controle ``addPriorConditions`` pode ser usado para impor qualquer tipo de restrição nos parâmetros do modelo durante a estimação. Por exemplo, para garantir a estimativa de uma covariância estacionária no modelo GARCH(1,1), a função deve ser definida como

```{r eval=FALSE }

addPriorConditions <- function(psi)
 psi[2] + psi[3] < 1

```

Finalmente, podemos impor a normalidade das perturbações de maneira direta, definindo os hiperparâmetros $\lambda = 100$ e $\delta = 500$ na função ``bayesGARCH``.

Então em nossos dados, teríamos a seguinte especificação:

```{r eval=FALSE }

MCMC_Milho <- bayesGARCH(retornos_Milho, lambda = 100, delta = 500,
                         control = list(n.chain = 2, l.chain = 20000,
                         addPriorConditions = addPriorConditions))

MCMC_Soja <- bayesGARCH(retornos_Soja, lambda = 100, delta = 500,
                         control = list(n.chain = 2, l.chain = 20000,
                         addPriorConditions = addPriorConditions))

```


# Conselho prático

A estratégia de estimativa implementada no pacote ``bayesGARCH`` é totalmente automática e não requer nenhum ajuste do amostrador MCMC. Isso é certamente um recurso atraente para os praticantes. A geração das cadeias de Markov é, no entanto, morosa em tempo consumido e se estimando o modelo ao longo de vários conjuntos de dados em uma base diária pode, portanto, levar uma quantidade  significativa de tempo. 

Neste caso, o algoritmo pode ser facilmente paralelizado, executando uma única cadeia em vários processadores. Isso pode ser facilmente alcançado com o pacote [foreach (REvolution Computing, 2010)](https://cran.r-project.org/web/packages/foreach/index.html), por exemplo. Além disso, quando a estimativa é repetida sobre séries temporais atualizadas (ou seja, séries temporais com mais observações recentes), é aconselhável iniciar o algoritmo
utilizando a média da _posteriori_ ou mediana dos parâmetros obtidos na etapa de estimação da _priori_. O impacto dos valores iniciais (fase de _burn in_) provavelmente será menor e, portanto, a convergência mais rápida.

Por fim, observe que, como qualquer algoritmo Metropolis-Hastings, o amostrador pode ficar preso em um determinado valor, de modo que a cadeia não se move mais. No entanto, o amostrador usa densidades candidatas feitas por Taylor que são especialmente construídas em cada etapa, então é quase impossível para este amostrador MCMC ficar preso em um dado valor para muitos sorteios subsequentes. Por exemplo, para nosso conjunto de dados ainda obtemos resultados das _posterioris_ que são quase iguais aos resultados que obtidos para os valores iniciais com razoáveis padrões
``c(0.01,0.1,0.7,20)``, mesmo se considerarmos os muito pobres valores iniciais ``c(0,1,0,01,0,4,50)``. No improvável caso esse mau comportamento ocorra, pode-se dimensionar os dados (para ter desvio padrão 1) ou executar o algoritmo com valores iniciais diferentes ou uma semente aleatória diferente.

















































&nbsp;

&nbsp;

&nbsp;

***

# Referências

***

Ardia, David (2008). _Financial Risk Management with Bayesian Estimation of GARCH Models: Theory and Application_. Lecture Notes in Economics and Mathematical Systems 612. Springer-Verlag, Berlin, Germany. ISBN 978-3-540-78656-6 doi:10.1007/978-3-540-78657-3

Ardia, David and Hoogerheide, Lennart F. (2010). _Bayesian estimation of the GARCH(1,1) model with Student-t innovations_. R Journal 2(2), pp.41-47 doi:10.32614/RJ-2010-014, Disponível em: [journal.r-project.org/](https://journal.r-project.org/archive/2010/RJ-2010-014/RJ-2010-014.pdf)

Ardia, David (2009). _Bayesian estimation of a Markov-switching threshold asymmetric GARCH model with Student-t innovations._ Econometrics Journal 12(1), pp.105-126. doi:10.1111/j.1368-423X.2008.00253.x

T. Bollerslev. _Generalized autoregressive conditional heteroskedasticity_. Journal of Econometrics, 31(3): 307–327, Apr. 1986.

P. J. Deschamps. _A flexible prior distribution for Markov switching autoregressions with_ _Student-t_
_errors_. Journal of Econometrics, 133(1):153–190, July 2006.

R. F. Engle. _Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation_. Econometrica, 50(4):987–1008, July 1982.

A. Gelman and D. B. Rubin. _Inference from iterative simulation using multiple sequences_. Statistical Science, 7(4):457–472, Nov. 1992.

J. F. Geweke. _Getting it right: Joint distribution tests of posterior simulators._ Journal of the American Statistical Association, 99(467):799–804, Sept. 2004.

J. F. Geweke. _Bayesian treatment of the independent Student-t linear model._ Journal of Applied Econometrics, 8(S1):S19–S40, Dec. 1993.

A. Ghalanos. ``rgarch``: _Flexible GARCH modelling in R_, 2010. URL http://r-forge.r-project.org/
projects/rgarch.

W. K. Hastings. _Monte Carlo sampling methods using Markov chains and their applications._
Biometrika, 57(1):97–109, Apr. 1970.

G. Koop. _Bayesian Econometrics_. Wiley-Interscience, London, UK, 2003. ISBN 0470845678.

N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. _Equations of state calculations by fast computing machines_. Journal of Chemical Physics, 21(6):1087–1092, June 1953.

T. Nakatsuma. _A Markov-chain sampling algorithm for GARCH models. Studies in Nonlinear Dynamics and Econometrics_, 3(2):107–117, July 1998. URL http://www.bepress.com/snde/vol3/iss2/algorithm1/. Algorithm nr.1.

M. Plummer, N. Best, K. Cowles, and K. Vines. CODA: _Convergence diagnosis and output analysis for MCMC_. R News, 6(1):7–11, Mar. 2006. 

M. Plummer, N. Best, K. Cowles, and K. Vines. coda: _Output analysis and diagnostics for MCMC_, 2010. URL http://CRAN.R-project.org/package=coda. R package version 0.13-5.

REvolution Computing. ``foreach``: _Foreach looping construct for R_, 2009. URL http://CRAN.R-project.org/package=foreach.

C. Ritter and M. A. Tanner. _Facilitating the Gibbs sampler: The Gibbs stopper and the_ _Griddy-Gibbs_ _sampler._ Journal of the American Statistical Association, 87(419):861–868, Sept. 1992.

A. Trapletti and K. Hornik. ``tseries``: _Time series analysis and computational finance_, 2009. URL http://CRAN.R-project.org/package=tseries.

D. Wuertz and Y. Chalabi. ``fGarch: Rmetrics`` - _Autoregressive Conditional Heteroskedastic Modelling_, 2009. URL http://CRAN.R-project.org/package=fGarch.


## Pacotes do R

```{r}

citation(package = "bayesGARCH")

citation(package = "coda")

```




&nbsp;

&nbsp;

***


Tempo total de execução do documento:

```{r}

end_time <- Sys.time()

end_time - start_time

```


