---
title: "Previsão (forecasting), simulação de cenários e análise de séries temporais"
author: "Rodrigo Hermont Ozon"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
params:
  symbol1: CORN
  symbol2: CN21.CBT
  symbol3: Cz21.CBT
  symbol4: CU21.CBT
  symbol5: CH22.CBT
  symbol6: CK22.CBT
  symbol7: ZC=F
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	comment = NA
)
knitr::opts_chunk$set(comment = NA) # Remove todos os coments # dos outputs do R
knitr::opts_chunk$set(warning = FALSE) # Remove todos os warnings # dos outputs do R
knitr::opts_chunk$set(message = FALSE) # Remove todas as mensagens # dos outputs do R

```


***

<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
font-style: italic;
}

</style>

<div class="alert alert-info">

  <strong>Economic Time Series Forecasting</strong> 
 
</div>



<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="comment">

Uma série temporal é um conjunto de observações dos valores que uma variável assume em diferentes
momentos do tempo.

Enquanto que o forecasting envolve fazer previsões sobre o futuro. É necessário em muitas situações: 

- decidir se construirá outra usina de geração de energia nos próximos dez anos requer previsões de demanda futura; 

- agendar equipe em um call center na próxima semana exige previsões de volumes de chamadas; 

- armazenar um estoque requer previsões de necessidades de estoque. 

As previsões podem ser exigidas com vários anos de antecedência (para o caso de investimentos de capital), ou apenas alguns minutos antes (para roteamento de telecomunicações). Quaisquer que sejam as circunstâncias ou horizontes de tempo envolvidos, a previsão (forecasting) é uma ajuda importante para um planejamento eficaz e inteligente.

</p>

***



![](https://rhozon.github.io/site/me.jpg)

```{r echo=FALSE}

# Libraries
library(igraph)
library(networkD3)
library(dplyr)

# A = Economia
# B = Econometria
# C = Microeconometria
# D = Modelos preditivos
# E = Métodos estatísticos
# K = Data Viz
# M = Data Science
# Z = Linguagem R
# Y = Séries Temporais (forecasting)


# create a dataset:
data <- data_frame(
  from=c("Economia", "Economia", "Econometria", "Modelos preditivos", "Microeconometria", "Modelos preditivos", "Métodos estatísticos", "Econometria", "Microeconometria", "Modelos preditivos", "Data Viz", "Economia", "Data Science", "Séries Temporais (forecasting)"),
  
  to=c("Econometria", "Métodos estatísticos", "Analytics", "Economia", "Microeconometria", "Economia", "Econometria", "Linguagem R", "Economia", "Microeconometria", "Economia", "Econometria", "Data Viz", "Econometria")
)

# Plot
p <- simpleNetwork(data, height="100px", width="100px",        
        Source = 1,                 # column number of source
        Target = 2,                 # column number of target
        linkDistance = 10,          # distance between node. Increase this value to have more space between nodes
        charge = -900,                # numeric value indicating either the strength of the node repulsion (negative value) or attraction (positive value)
        fontSize = 14,               # size of the node names
        fontFamily = "serif",       # font og node names
        linkColour = "#666",        # colour of edges, MUST be a common colour for the whole graph
        nodeColour = "#69b3a2",     # colour of nodes, MUST be a common colour for the whole graph
        opacity = 0.9,              # opacity of nodes. 0=transparent. 1=no transparency
        zoom = T                    # Can you zoom on the figure?
        )

p


```


***

$$\\[1in]$$


# Introdução


```{r include = FALSE}

library(quantmod)
library(dygraphs)

CORN <- getSymbols(params$symbol1, auto.assign = FALSE,
                    from = "2018-01-01", end = Sys.Date())

CN21.CBT <- getSymbols(params$symbol2, auto.assign = FALSE,
                    from = "2018-01-01", end = Sys.Date())

Cz21.CBT <- getSymbols(params$symbol3, auto.assign = FALSE,
                    from = "2018-01-01", end = Sys.Date())

CU21.CBT <- getSymbols(params$symbol4, auto.assign = FALSE,
                    from = "2018-01-01", end = Sys.Date())

CH22.CBT <- getSymbols(params$symbol5, auto.assign = FALSE,
                    from = "2018-01-01", end = Sys.Date())

CK22.CBT <- getSymbols(params$symbol6, auto.assign = FALSE,
                    from = "2018-01-01", end = Sys.Date())

ZC_F <- getSymbols(params$symbol7, auto.assign = FALSE,
                    from = "2018-01-01", end = Sys.Date())

```



Esse relatório capta dinamicamente as cotações dos seguintes ativos relacionados as negociações da _commoditie_ milho:
  
* ``r params$symbol1`` [CORN (Teucrium Corn Fund)](https://finance.yahoo.com/quote/CORN?p=CORN) $\Rightarrow$ <mark>última cotação ``r Sys.Date()``.</mark>

* ``r params$symbol2`` [CN21.CBT](https://finance.yahoo.com/quote/CN21.CBT/history?p=CN21.CBT) $\Rightarrow$ **Não utilizado como objeto de análise aqui ainda**

* ``r params$symbol3`` [Cz21.CBT](https://finance.yahoo.com/quote/CZ21.CBT/) $\Rightarrow$ **Não utilizado como objeto de análise aqui ainda**

* ``r params$symbol4`` [CU21.CBT](https://finance.yahoo.com/quote/CU21.CBT/) $\Rightarrow$ **Não utilizado como objeto de análise aqui ainda**

* ``r params$symbol5`` [CH22.CBT](https://finance.yahoo.com/quote/CH22.CBT/) $\Rightarrow$ **Não utilizado como objeto de análise aqui ainda**

* ``r params$symbol6`` [CK22.CBT](https://finance.yahoo.com/quote/CK22.CBT/) $\Rightarrow$ **Não utilizado como objeto de análise aqui ainda**

* ``r params$symbol7`` [ZC=F](https://finance.yahoo.com/quote/ZC%3DF/history?p=ZC%3DF) $\Rightarrow$ **Não utilizado como objeto de análise aqui ainda**

Os dados são oriundos do [Yahoo finance](http://finance.yahoo.com). As séries temporais se iniciam em 01/01/2018 e terminam no último valor de fechamento disponível na série, quase que em _real time_.


***



Carregando as bibliotecas necessárias

```{r}

library(zoo)
library(TSstudio)
library(timetk)
library(lmtest)
library(quantmod)
library(dygraphs)
library(tidyverse)
library(dplyr)
library(plotly)
library(ggplot2)
library(forecast)
library(astsa)
library(prophet)
library(DT)
library(fpp2)
library(tsbox)
library(lubridate)
library(data.table)
library(tseries)

```


_Uma série temporal é um conjunto de observações dos valores que uma variável assume em diferentes momentos do tempo._ Esses dados podem ser coletados a intervalos regulares, como diariamente (preços das ações, relatórios meteorológicos), semanalmente (informações sobre oferta de moeda), mensalmente (taxa de desemprego, índice de preços ao consumidor [IPC]), trimestralmente (PIB), anualmente (orçamento do governo), quinquenalmente, isto é, a cada cinco anos (censo industrial dos Estados Unidos), ou decenalmente (censo demográfico). Às vezes, os dados estão disponíveis em séries trimestrais e anuais, como no caso do PIB e das despesas de consumo. Com o advento dos computadores de alta velocidade, _os <mark>dados agora podem ser coletados a intervalos extremamente curtos, como os relativos a preços das ações, obtidos de forma praticamente contínua (as chamadas cotações em tempo real)</mark>._

A maioria dos estudos empíricos na econometria das séries temporais embasados nesse tipo de dado pressupõe que a série temporal subjacente seja estacionária. De maneira _geral, <mark>uma série é estacionária se sua média e variância não variam sistematicamente ao longo do tempo</mark>,_
ou em termos não exatos, uma série temporal, por exemplo, $Y_t$, é estacionária se sua média e sua variância não mudam sistematicamente ao longo do tempo.

```{r}

CORN <- getSymbols("CORN", auto.assign = FALSE,
                    from = "1994-01-01", end = Sys.Date())

data_inicio <- start(CORN)
data_inicio
data_fim <- end(CORN)
data_fim

```



```{r fig.width=8, fig.height=3}

glimpse(CORN) # Perfil dos dados

# Função log retornos
ret<-function(x,k=1){
  return(diff(log(x),k))
}

grupo <- cbind(Cl(CORN), ret(Cl(CORN)), Vo(CORN), ret(Vo(CORN)))

dygraph(Cl(CORN), group = "grupo") %>% dyRangeSelector() # Fechamento
dygraph(ret(Cl(CORN)), group = "grupo") %>% dyRangeSelector() # retornos do Fechamento

dygraph(Vo(CORN), group = "grupo") %>% dyRangeSelector()  # Volume negociado
dygraph(ret(Vo(CORN)), group = "grupo") %>% dyRangeSelector() # retornos do volume

```

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

"Por que as séries temporais estacionárias são tão importantes? Porque, se uma série temporal é não estacionária, podemos estudar seu comportamento apenas pelo período de tempo em consideração. Cada conjunto de dados de série temporal, portanto, será específico a cada episódio. Como consequência, não é possível generalizá-lo para outros períodos. Sendo assim, para o propósito de previsão, tal série temporal (não estacionária) pode ser de pouco valor prático." (Gujarati e Porter, p. 735, 2011)

</p>


```{r}

# Manipulo pra selecionar as variaveis necessarias 
#CORN_diaria <- tk_zooreg(CORN, start = 2010, freq = 365, silent = TRUE) 
#head(CORN)

CORN_diaria <- ts(CORN, start = decimal_date(as.Date("2010-06-09")), end = decimal_date(as.Date(Sys.Date())), frequency = 365) # Mantenho a serie diaria para usos posteriores

CORN_df <- as.data.table(CORN)
glimpse(CORN_df)

CORN_df <- CORN_df %>%
  rename(Date = "index")

glimpse(CORN_df)

```

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

"Na análise aplicada, os dados brutos muitas vezes são “manipulados”. Por exemplo, em regressões de séries temporais que envolvem dados trimestrais, muitas vezes os dados são obtidos somando três observações mensais e dividindo a soma por três. Essas médias suavizam os dados amenizando as flutuações dos dados mensais. Portanto, a representação gráfica dos dados trimestrais é muito menos irregular que a dos dados mensais e essa mesma regularidade pode gerar um padrão sistemático nos termos de erro, introduzindo a autocorrelação." (Gujarati e Porter, p. 419, 2006)

</p>


```{r fig.width=9, fig.height=3}

# Transformo em serie temporal diária para mensal

CORN_mensal <- ts(CORN_df[-1], start = c(2010, 6), end = c(2021, 5), frequency = 12)

head(CORN_mensal) # Primeiras 5 observações do conjunto de dados

tail(CORN_mensal) # Ultimas 5 observações do conjunto de dados

glimpse(CORN_mensal)

colnames(CORN_mensal) # Mostre somente os nomes das colunas

```


## Causalidade x modelos de regressão em séries temporais

Embora a análise de regressão lide com a dependência de uma variável sobre outras variáveis, ela não implica necessariamente causação. Nesses modelos, uma premissa básica é que a relação de causa e efeito, se houver, entre o $Y$ e os $X$ é unidirecional. As variáveis explanatórias são a causa e a variável dependente é o "efeito".

Entretanto, há situações nas quais existe um fluxo de influência de mão dupla entre as variáveis econômicas; ou seja, uma variável econômica afeta outra(s) variável(eis) econômica(s) e é, por sua vez, afetada por ela(s).

Em outras palavras, a existência de uma relação entre variáveis não prova causalidade ou a direção da influência. Mas, em regressões envolvendo dados de séries temporais, a situação pode ser um pouco diferente, porque, como coloca o autor,


<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

[. . .] o tempo não volta. Ou seja, se o evento A acontece antes do evento B, então é possível que A esteja causando B. No entanto, não é possível que B esteja causando A. Em outras palavras, os eventos passados podem levar ao acontecimento de eventos no presente. Os eventos futuros não podem. (Koop, p. 175, 2000)

</p>

Essa é a ideia aproximada do chamado teste de causalidade de Granger (Granger, p. 424-438, 1969). Mas deve-se observar que a questão da causalidade é profundamente filosófica, com todos os tipos de controvérsias. Em um extremo estão as pessoas que acreditam que _“tudo tenha uma causa”,_ e no outro estão aquelas que negam a existência de causação, seja qual for. O econometrista Edward Leamer prefere o termo precedência a causalidade. Francis Diebold prefere o termo causalidade preditiva. Como ele escreve:

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

[. . .] a afirmação “ yi causa yj” é uma abreviação da afirmação mais exata, porém mais longa: “yi contém informações úteis para prever yj (no sentido dos mínimos quadrados lineares), acima e além das histórias passadas das outras variáveis no sistema”. Para poupar espaço, dizemos simplesmente que yi causa yj.

</p>

A justificativa do uso do teste de causalidade se fundamenta na busca da seguinte resposta para a direção de causa e efeito entre preço do ativo ``CORN`` e quantidade transacionada dele próprio:

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

Se, em um sistema de equações simultâneas que contenha duas ou mais equações, não for possível obter valores numéricos de cada parâmetro em cada equação, porque as equações são empiricamente indistinguíveis, ou muito parecidas, temos o problema da identificação. Sendo assim, na regressão da quantidade Q sobre o preço P, a equação resultante é uma função de demanda ou uma função de oferta (Q e P fazem parte de ambas as funções)? Se tivermos apenas dados sobre Q e P e nenhuma outra informação, será difícil, senão impossível, identificar a regressão como uma função de demanda ou oferta. É fundamental resolvermos o problema da identificação antes de procedermos à estimação, porque, se não sabemos
o que estamos estimando, a estimação per se não tem sentido.

</p>

### Teste de Causalidade de Granger

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

O teste da causalidade de Granger pressupõe que as informações relevantes à previsão das respectivas variáveis preditivas, (p. ex. duas em duas como PIB e M), estão contidas unicamente nos dados de série temporal dessas variáveis. (Gujarati e Porter, p. 647-653)

Em que se supõe que os termos de erro $u_{1t}$ e $u_{2t}$ não estejam correlacionados. A propósito, observe que, uma vez que temos duas variáveis, estamos lidando com a causalidade bilateral. Nas demais séries temporais econométricas, estenderemos isso à causalidade multivariada através da técnica de vetores autorregressivos (VAR).

</p>

1. Uma _causalidade unidirecional_ de M para PIB será indicada se os coeficientes estimados das defasagens de M forem estatisticamente diferentes de zero como grupo e o conjunto de coeficientes estimados do PIB não for estatisticamente diferente de zero.

2. Por outro lado, a causalidade _unidirecional_ do PIB a M existe se o conjunto de coeficientes defasados não é estatisticamente diferente de zero e o conjunto dos coeficientes do PIB é estatisticamente diferente de zero.

3. _Feedback, ou causalidade bilateral,_ será sugerido quando os conjuntos de coeficientes de M e PIB forem estatisticamente diferentes de zero em ambas as regressões.

4. Por fim, a independência será sugerida quando os conjuntos de coeficientes de M e PIB não forem estatisticamente significativos em nenhuma das regressões.

Você pode encontrar um maior detalhamento [de modelagem econométrica utilizando Inferência Causal aqui](https://rhozon.github.io/selecaodeprodutosnor/inferenciacausal.html) ou então nas [literaturas recomendadas](https://book4you.org/s/Causal%20Inference)


```{r}

# Os preços (Granger) causam o volume ?

grangertest(CORN[,"CORN.Volume"] ~ CORN[,"CORN.Close"], order=4)

```

*Existem várias maneiras de encontrar o _lag_ ideal, que vou pular por causa do tempo, mas digamos que quatro seja um número mágico. (hehe)

O valor$-p$ é altamente significativo, mas e na outra direção ?

```{r}

# O volume Granger causa os precos ?

grangertest(CORN[,"CORN.Close"] ~ CORN[,"CORN.Volume"], order=4)

```


Como ele não é significativo, então podemos dizer que conforme o resultado do primeiro teste de causalidade, _.<mark>os preços Granger causam o volume de negociação do ativo ``CORN`` (milho)</mark>._

O sentido econômico disso se fundamenta na premissa da hipótese de mercados eficientes. Na definição clássica dessa hipótese, Fama (1970) definiu _"mercado financeiro eficiente como aquele em que o preço dos ativos negociados sempre reflete inteiramente as informações disponíveis sobre os mesmos."_

O poder dessa Hipótese é significativo, pois a HME descarta a possibilidade de ganhos consistentes com sistemas de negociação que se baseiem apenas nas informações disponíveis. Conforme Shleifer (2000), um investidor médio, seja ele indivíduo, fundo de pensão ou fundo mútuo, não deve esperar superar o mercado consistentemente, e os recursos que tais tipos de investidores utilizam para analisar e negociar ativos são desperdiçados, sendo melhor manter, passivamente, a carteira de mercado, esquecendo a gestão ativa de carteiras. Pode-se afirmar, seguramente, que o campo acadêmico das finanças em geral e, especificamente, o campo de análise e precificação de ativos foi construído com base na HME.

Afirmar, portanto, que um mercado é eficiente em termos de informação significa que não há maneiras de obtenção de lucros anormais mediante o uso da informação, visto que os preços já contemplam essa informação.

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

"Aqueles que acreditam na hipótese de eficiência do mercado de capital argumentam que os preços das ações são essencialmente aleatórios e, por conseguinte, não há margem para especulação lucrativa no mercado de ações: se fosse possível prever o preço de amanhã com base no preço de hoje, todos seríamos milionários." (Gujarati e Porter, p. 736, 2011)

</p>

Esta é apenas a ponta do iceberg, mas deve ser suficiente para despertar sua curiosidade e torná-lo perigoso.


<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

Em termos mais gerais, uma vez que o futuro não pode prever o passado, se a variável X (Granger) causa a variável Y, variações em X deveriam preceder variações em Y. Portanto, em uma regressão de Y sobre outras variáveis (incluindo seus próprios valores passados), se incluirmos os valores passados ou defasados de X e ele aprimorar significativamente a previsão de Y, poderemos dizer que X (Granger) causa Y. Uma definição similar aplica-se se Y (Granger) causa X.

</p>


## Transformação das séries

Um dos objetivos em finanças é avaliação de riscos de uma carteira de ativos (instrumentos) financeiros. O risco é frequentemente medido em termos de variações de preços dos ativos.

Denotemos por $P_t$ o preço de um ativo no instante $t$, normalmente um dia de negócio. Suponha, primeiramente, que não haja dividendos pagos no
período. A variação de preços entre os instantes $t-1$ e $t$ é dada por $\Delta P_t = P_t - P_{t-1}$ e a variação relativa de preços ou retorno líquido simples deste ativo entre os mesmos instantes é definido por:


$$
R_t = \displaystyle\frac{P_t-P_{t-1}}{P_t}
$$
Note que $R_t = P_t/P_{t-1}$: Chamamos $1 + R_t = P_t-P_{t-1}$ de retorno bruto simples. Usualmente expressamos $R_t$ em percentagem, relativamente ao período (um dia, um mês, um ano, etc).É também chamado de taxa de retorno.

Denotando $pt = log Pt$ (sendo o logaritmo na base $e$), definimos _o retorno composto continuamente_ ou simplesmente log-retorno como

$$
r_t = log\frac{P_t}{P_{t-1}} = log(1+R_t) = p_t - p_{t-1}
$$


Esta definição será aquela comumemente utilizada e, muitas vezes, $r_t$ será chamado simplesmente de retorno.

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

"Na prática é preferivel trabalhar com retornos, que são livres de escala, do que com preços, pois os primeiros têm propriedades estatísticas mais interessantes (como estacionariedade e ergodicidade). Um dos objetivos será, então, modelar retornos. Diversas classes de modelos podem ser utilizadas para esse fim, tais como os modelos ARMA, ARCH, GARCH, modelos de volatilidade estocástica, etc." (Morettin, p. 8, 2006)

</p>

Crio uma função que obtém os retornos logarítmicos para um período:

```{r}

# Função log retornos
ret<-function(x,k=1){
  return(diff(log(x),k))
}

retornos <- ret(Cl(CORN))

retornos_volume <- ret(Vo(CORN))

serie_retornos <- data.frame(retornos, retornos_volume)

serie_retornos <- ts(serie_retornos, start = decimal_date(as.Date("2010-06-10")), end = decimal_date(as.Date(Sys.Date())), frequency = 365) 

# https://stackoverflow.com/questions/31491157/how-do-i-create-daily-time-series-starting-from-a-specific-date/31491208

glimpse(serie_retornos)

head(serie_retornos) # Primeiros registros da série

tail(serie_retornos) # Ultimos registros da série

```

Note que ao delimitar a data de início da série temporal inclui um dia a mais em função da perda da primeira observação pela transformação em retorno das séries. 

Vamos investigar o comportamento das séries de retorno no tempo:


```{r fig.width=9, fig.height=3}

# Gráfico dos retornos do preço e retornos do volume

ggplotly(
  autoplot(serie_retornos[,"CORN.Close"]) # Gráfico dos retornos das cotações de fechamento 
      )

ggplotly(
  autoplot(serie_retornos[,"CORN.Volume"]) # Gráfico dos retornos do volume de negociação 
      )

```




```{r fig.width=9, fig.height=3}

ggplotly(
ggsubseriesplot(CORN_mensal[, "CORN.Close"]) +
  ylab("US$/bushel") +
  ggtitle("Seasonal subseries plot: CORN.Close")) # Inspeção do padrão sazonal da série de preços

ggplotly(
ggsubseriesplot(CORN_mensal[, "CORN.Volume"]) +
  ylab("US$/bushel") +
  ggtitle("Seasonal subseries plot: CORN.Volume")) # Inspeção do padrão sazonal da série de volume


ggplotly(
  ggplot(as.data.frame(CORN), aes(x = CORN.Volume, y = CORN.Close))+
  geom_point()+
  geom_smooth(method = "lm", se = TRUE)+ # Ajusta uma regressao linear simples para demonstrar o relacionamento das variaveis em nível
 ggtitle("Ajuste de curva de relacionamento linear entre Preço e Volume"))

ggplotly(
  ggplot(as.data.frame(serie_retornos), aes(x = CORN.Volume, y = CORN.Close))+
  geom_point()+
  geom_smooth(method = "lm", se = TRUE)+ # Ajusta uma regressao linear simples para demonstrar o relacionamento das variaveis em nível
 ggtitle("Ajuste de curva de relacionamento linear entre os retornos de Preço e Volume"))


GGally::ggpairs(as.data.frame(CORN[,c("CORN.Close","CORN.Volume")])) # Padrões de correlação

```

Assim como explicado da necessidade de trabalharmos com retornos que são livres de escala, explicado por Morettin, 2006, de maneira análoga explicam os autores:

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

"uma vantagem secundária da transformação de primeira diferença é que ela pode tornar estacionária uma série temporal não estacionária." (Gujarati e Porter, p. 352, p. 352)

</p>


# Padrão de estacionariedade das séries

A autocorrelação pode ser definida como _"correlação entre integrantes de séries de observações ordenadas no tempo [como as séries temporais]"_ ou no espaço [como nos dados de corte transversal]. Convém notar também que a autocorrelação pode ser tanto positiva quanto negativa, embora a maior parte das séries temporais econômicas em geral apresente autocorrelação positiva, pois, em sua maioria, evolui para cima ou para baixo por longos períodos e não apresenta oscilações constantes.

![](https://github.com/rhozon/datasets/raw/master/padroes_autocorrelacao.png)
<small> **Fonte:** Gujarati e Porter (p. 417, 2006) </small>

Em análise de séries temporais, pois as observações de tais dados seguem um ordenamento natural, de modo que observações sucessivas costumam
apresentar intercorrelações, especialmente se o intervalo de tempo entre observações sucessivas for curto, como um dia, uma semana ou um mês, e não um ano. Quando observamos índices de preços de ações, como o Dow Jones ou o S&P 500, durante dias sucessivos, não é raro verificar que esses
índices sobem ou descem por vários dias seguidos. Obviamente, em situações como essa, a hipótese de ausência de autocorrelação ou ausência de correlação serial nos termos de erro que embasa o modelo clássico de regressão linear não será respeitada.

```{r fig.width=9, fig.height=3}

 # Funções de autocorrelação
ggplotly(
  ggAcf(CORN[,"CORN.Close"], na.action = na.interp) # Fechamento
)

ggplotly(
  ggAcf(CORN[,"CORN.Volume"], na.action = na.interp) # Volume
)

```


## Testes de raiz unitária a integração das séries

Algumas vezes, a autocorrelação ocorre, porque a série temporal subjacente é não estacionária.

De maneira contrária, a série temporal é estacionária se suas características (por exemplo, a média, variância e covariância) não variam ao longo do tempo.


```{r fig.width=9, fig.height=3}

adf.test(CORN[,"CORN.Close"]) # Dickey-Fuller Ampliado para fechamento
adf.test(CORN[,"CORN.Volume"]) # Dickey-Fuller Ampliado para volume

Acf(CORN[,"CORN.Close"]) # Função de autocorrelação
pacf(CORN[,"CORN.Close"], na.action = na.pass) # Função de autocorrelação parcial
Acf(serie_retornos[,"CORN.Close"]) # Função de autocorrelação na primeira diferença
pacf(serie_retornos[,"CORN.Close"], na.action = na.pass) # Função de autocorrelação parcial na primeira diferença

```

```{r fig.width=9, fig.height=3}

Acf(CORN[,"CORN.Volume"]) # Função de autocorrelação
pacf(CORN[,"CORN.Volume"],  na.action = na.pass) # Função de autocorrelação parcial
Acf(serie_retornos[,"CORN.Volume"],  na.action = na.pass) # Função de autocorrelação na primeira diferença
pacf(serie_retornos[,"CORN.Volume"],  na.action = na.pass) # Função de autocorrelação parcial na primeira diferença

```

Teste Dickey-Fuller Ampliado para séries em primeiras diferenças

```{r eval=FALSE}

ret_fech_sem_na <- !is.na(serie_retornos[,"CORN.Close"])
ret_vol_sem_na <- !is.na(serie_retornos[,"CORN.Volume"])

adf.test(ret_fech_sem_na[,"CORN.Close"]) # retornos do fechamento
adf.test(ret_vol_sem_na[,"CORN.Volume"]) # retornos do volume

```

A saída do ``adf.test`` inclui um valor $p$. Convencionalmente, se $p <0,05$, a série temporal provavelmente está se revertendo à média, enquanto que se $p> 0,05$ não fornece tal evidência.

De modo similar, testamos o processo de reversão à média se evidente nas séries:

```{r eval=FALSE}

pp.test(CORN[,"CORN.Close"], type = "Z(t_alpha)") # Phillips-Perron em nível
pp.test(CORN[,"CORN.Volume"], type = "Z(t_alpha)") 

pp.test(diff(CORN[,"CORN.Close"]), type = "Z(t_alpha)")# PP em primeira diferencas
pp.test(diff(CORN[,"CORN.Volume"]), type = "Z(t_alpha)")

```

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

Em resumo, se uma série temporal for estacionária, a média, variância e autocovariâncias (em variadas defasagens) permanecerão
as mesmas não importa em que ponto a mensuremos; isto é, elas serão invariantes no tempo. Tal série temporal tenderá a retornar para a sua média (o que chamamos de reversão da média), e flutuações em torno dessa média (mensurada por sua variância) terão, de modo geral, uma amplitude constante.

Em outras palavras, um processo estacionário não se desviará muito de seu valor médio em virtude da variância finita. Como veremos em breve, esse não é o caso do processo estocástico não estacionário. Devemos observar que, para o processo estacionário, a velocidade da reversão à média
depende das autocovariâncias; isso será rápido, se as autocovariâncias forem pequenas, e lento quando são grandes. (Gujarati e Porter, p. 735, 2011)

</p>


## Teste de estacionariedade

```{r fig.width=9, fig.height=3}

kpss.test(CORN[,"CORN.Close"]) # Teste KPSS em nivel
kpss.test(CORN[,"CORN.Volume"]) 

kpss.test(diff(CORN[,"CORN.Close"])) # Teste KPSS em primeira diferenca
ggAcf(diff(CORN[,"CORN.Close"])) # Função de autocorrelação em primeira diferenca

kpss.test(diff(CORN[,"CORN.Volume"]))
ggAcf(diff(CORN[,"CORN.Volume"])) 

```


Portanto, o teste KPSS também aponta para a não estacionariedade da série de preços e de volume. (Novamente, um aviso é emitido, pois o valor $p$ é interpolado dos quatro valores críticos fornecidos por Kwiatkowski _et al._ 1992, ele é suprimido aqui.)


```{r}

Box.test(CORN[,"CORN.Close"], lag = 24, fitdf = 0, type = "Lj")
Box.test(CORN[,"CORN.Volume"], lag = 24, fitdf = 0, type = "Lj")

```

um valor $p$ maior que 0,05 sugere que os dados não são significativamente diferentes do ruído branco. (ou seja, se o valor$-p$ da Ljung-Box for maior que 0,05 ou 5% temos uma série do tipo ruído branco)

Testaremos a correlação entre volume e preço do ativo ``CORN``

```{r fig.width=9, fig.height=3, eval=FALSE}

Ccf(diff(CORN[,"CORN.Close"]), diff(CORN[,"CORN.Volume"]), main = "Fechamento vs. Volume")

```

Cada linha vertical mostra a correlação entre as duas séries temporais em algum atraso, conforme indicado ao longo do eixo x. Se uma correlação se estender acima ou abaixo das linhas pontilhadas, ela é estatisticamente significativa.

Observe que a correlação no lag 12 é de aproximadamente –0,11, que é a correlação simples entre as variáveis. Evidentemente, há algum “efeito cascata” no volume do dia-a-dia e no preço das commodities porque as mudanças hoje estão correlacionadas com as mudanças amanhã. Descobrir esse tipo de relacionamento é útil para analistas de curto prazo, como analistas de mercado, _traders_ e corretores de títulos, p. ex.

## Avaliação da presença de cointegração entre volume e preço

A própria natureza das duas séries já sugere que elas possuem características comuns. Tendo evidência de não estacionariedade, é interessante testar um componente não estacionário comum por meio de um teste de cointegração. Um método simples para testar a cointegração é o método de duas etapas proposto por Engle e Granger (1987). Ele faz a regressão de uma série na outra e executa um teste de raiz unitária nos resíduos. Este teste, muitas vezes nomeado após Phillips e Ouliaris (1990), que forneceram a teoria assintótica, está disponível na função ``po.test()`` do pacote ``tseries.``

```{r}

#po.test(log(CORN[,5:6]))

```

Como esperadamente, o resultado do teste demonstra que a cointegração das séries em nível (logarítmico) não se fez presente.

Os testes padrão que procedem de maneira simétrica derivam da abordagem de máxima verossimilhança de informações completas de Johansen (Johansen, 1991). Para um modelo de vetor autorregressivo (VAR) cointegrado de ordem $p$, a forma de correção de erro é (omitindo componentes determinísticos)

Os testes relevantes estão disponíveis na função ``ca.jo()`` do pacote ``urca``. Vide p. ex. Hamilton (1994) para o background metodológico. Aqui, empregamos a estatística de rastreamento - o teste de autovalor máximo, ou ``“lambdamax”``, também está disponível - em uma equação corrigida por um termo constante (especificado por ``ecdet = "const")``, resultando

TESTE JJ AQUI


Todavia, a hipótese nula de nenhuma cointegração é rejeitada; portanto, o teste de Johansen confirma os resultados da abordagem inicial em duas etapas.


## Teste de Quebra Estrutural

O teste de mudança estrutural associado, por padrão considerando o desvio máximo absoluto do processo de flutuação empírica em torno de zero é tido como significativo no nível padrão de 5%, sinalizando que os parâmetros do modelo não são estáveis ao longo de todo o período da amostra.

```{r fig.width=9, fig.height=3}

library(strucchange)

dd_ocus <- efp(log(CORN[,"CORN.Close"]) ~ log(CORN[, "CORN.Volume"]), data = CORN, type = "OLS-CUSUM")
sctest(dd_ocus)
plot(dd_ocus)

```

Testes baseados em estatísticas $F$, a segunda classe de testes em mudança estrutural, são projetados para ter boa potência para alternativas de quebra única (de tempo desconhecido). A ideia básica é calcular uma estatística $F$ (ou estatística/teste Chow) para cada ponto de interrupção/quebra concebível em um determinado intervalo e rejeitar a hipótese nula de estabilidade estrutural se alguma dessas estatísticas (ou algum outro funcional, como a média) exceder um certo valor crítico (Andrews 1993; Andrews e Ploberger 1994). Processos de estatísticas $F$ podem ser ajustados com ``Fstats()``, empregando uma interface semelhante a ``efp()``. Os objetos “Fstats” resultantes podem ser avaliados novamente pelo método ``sctest()`` correspondente ou graficamente pelo método ``plot()``. O pedaço de código


```{r fig.width=9, fig.height=3}

dd_fs <- Fstats(log(CORN[,"CORN.Close"]) ~ log(CORN[, "CORN.Volume"]), data = CORN, from = 0.1)
plot(dd_fs)
sctest(dd_fs)

```

Portanto, não rejeitaremos a hipótese nula de estabilidade dos parâmetros (ausência de mudança estrutural) se o valor de $F$ calculado em uma aplicação não for superior ao valor de $F$ crítico registrado na tabela F no nível de significância (ou valor $p$) escolhido.

## Modelo Dating Structural Changes

Voltando à série ``CORN``, estimamos os pontos de interrupção para um modelo do tipo SARIMA com um tamanho mínimo de segmento de 10% usando,

```{r fig.width=9, fig.height=5}

dd_bp <- breakpoints(log(CORN[,"CORN.Close"]) ~ log(CORN[, "CORN.Volume"]), data = CORN, h = 0.1)
plot(dd_bp)

```

O RSS e o BIC conforme exibidos pelo gráfico (``dd_bp``) são mostrados a seguir. Embora o RSS caia claramente até ``m = 3`` quebras, o BIC é mínimo para ``m = 0`` quebras. Isso não é muito satisfatório, pois os testes de mudança estrutural mostraram claramente que os parâmetros do modelo não são estáveis. Como o BIC foi considerado pouco confiável para modelos autorregressivos por Bai e Perron (2003), contamos com a interpretação da visualização dos testes de mudança estrutural e usamos o modelo com ``m = 2`` quebras. Seus coeficientes podem ser extraídos via


```{r}

coef(dd_bp, breaks = 2)

```

As séries observadas e ajustadas, juntamente com os intervalos de confiança para os pontos de quebra, são mostradas a seguir conforme gerado por

```{r fig.width=9, fig.height=5}

plot(log(CORN[,"CORN.Close"]))
lines(fitted(dd_bp, breaks = 2), col = 4)
lines(confint(dd_bp, breaks = 2))

```


## Análise da Estrutura da Série Temporal

Os modelos de estrutura de séries temporais são modelos de espaço de estado que utilizam uma decomposição da série temporal em uma série de componentes que são especificados por um conjunto de variações de perturbação. Assim, esses modelos podem ser considerados modelos de componentes de erro para dados de séries temporais. Harvey (1989) e Durbin e Koopman (2001) são referências padrão.

```{r fig.width=9, fig.height=10}

dd_struct_Close <- StructTS(log(CORN[,"CORN.Close"]))

plot(cbind(fitted(dd_struct_Close), residuals(dd_struct_Close)))

dd_struct_Vol <- StructTS(log(CORN[,"CORN.Volume"]))

plot(cbind(fitted(dd_struct_Vol), residuals(dd_struct_Vol)))


```


# Testagem de Modelos de Previsão

```{r}

treino <- window(log(CORN[,c("CORN.Close","CORN.Volume")]), start = c(2007,1), end = c(2019,4)) # Pego o período onde o gráfico aponta inversão de trajetoria ate os ultimos 24 meses anteriores ao final da serie

teste <- window(log(CORN[,c("CORN.Close","CORN.Volume")]), start = c(2019,5)) # Conjunto de teste contempla aprox os ultimos 24 meses em diante

glimpse(treino)

```

Construo os modelos para avaliação de performance preditiva:

```{r}

tail(treino)

horizonte <- 24 # Horizonte de projeção para os proximos 24 meses

```

## SNAIVE

De acordo com Hyndman & Anastopoulous (p. 57, 2018):

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

For naïve forecasts, we simply set all forecasts to be the value of the last observation. That is,

</p>

$$
\widehat{y}_{T+h|T}=y_{T}
$$

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

This method works remarkably well for many economic and financial time series.

Because a naïve forecast is optimal when data follow a random walk, these are also called random walk forecasts.

A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season of the year (e.g., the same month of the previous year). Formally, the forecast for time T+h is written as 

</p>

$$
\widehat{y}_{T+h|T}=y_{T+h-m(k+1)},
$$

_where $m=$ the seasonal period, and $k$ is the integer part of $(h-1)/m$ (i.e., the number of complete years in the forecast period prior to time $T+h$). This looks more complicated than it really is. For example, with monthly data, the forecast for all future February values is equal to the last observed February value. With quarterly data, the forecast of all future Q2 values is equal to the last observed Q2 value (where Q2 means the second quarter). Similar rules apply for other months and quarters, and for other seasonal periods._

```{r fig.width=9, fig.height=3}

SNAIVE <- snaive(diff(CORN[,"CORN.Close"]), h = horizonte)
#summary(SNAIVE)
autoplot(SNAIVE)+autolayer(fitted(SNAIVE))
checkresiduals(SNAIVE)

```

## Random Walk com drift

Quando se assume que o comportamento dos preços de um ativo segue a hipótese de _Random Walk_, diz-se que ele obedece ao Processo de Markov, um processo estocástico em que o comportamento de uma variável durante um período curto de tempo depende somente do valor da variável no início do período, e não de seu histórico. O preço dos títulos, principalmente das ações, é tido como um processo de Markov. Se isso for verdadeiro, como salienta Hull, nossas previsões sobre o futuro dos títulos não devem ser afetadas pelos preços dos mesmos há uma semana, um mês ou mesmo um ano atrás. Previsões para o futuro são incertas e devem ser expressas em termos de distribuições probabilísticas.

A estimação deste modelo é definida por Hyndman & Anastopoulous (p. 57, 2018):

A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data. Thus the forecast for time is given by

$$
\widehat{y}_{T+h|T}=y_{T}+ \frac{h}{T-1}\displaystyle\sum^{T}_{t=2}(y_{t}-y_{t-1})= y_{T}+h (\frac{y_{T}-y_{1}}{T-1}) 
$$
This is equivalent to drawing a line between the first and last observations, and extrapolating it into the future.

```{r fig.width=9, fig.height=3}

RW_drift <- rwf(diff(CORN[,"CORN.Close"]), h = horizonte, drift=TRUE)

autoplot(RW_drift)+autolayer(fitted(RW_drift))
checkresiduals(RW_drift)

```

Gujarati e Porter (p. 731, 2011) define o fenômeno como:

<p >
<p style="font-family: times, serif; font-size:11pt; font-style:italic"; class="quote">

"(...) algumas séries temporais financeiras, como os preços das ações, exibem o que é conhecido como fenômeno do passeio aleatório. Isso significa que a melhor previsão do preço de uma ação, por exemplo, da IBM, amanhã seja igual ao preço de hoje mais um choque puramente aleatório (ou termo de erro). Se esse for realmente o caso, prognosticar os preços dos ativos seria um exercício inútil."

</p>

## SES

```{r fig.width=9, fig.height=3}

SES <- ses(diff(CORN[,"CORN.Close"]), h = horizonte) # Simple Exponential Smoothing

autoplot(SES)+autolayer(fitted(SES))
checkresiduals(SES)

```

## Holt

```{r fig.width=9, fig.height=3}

HOLT <- holt(diff(CORN[,"CORN.Close"]), h = horizonte) 

autoplot(HOLT)+autolayer(fitted(HOLT))
checkresiduals(HOLT)

HOLT_aditivo <- hw(diff(CORN[,"CORN.Close"]), seasonal = "additive", h = horizonte)

autoplot(HOLT_aditivo)+autolayer(fitted(HOLT_aditivo))
checkresiduals(HOLT_aditivo)

```

## ARIMA

```{r fig.width=9, fig.height=3}

ARIMA <- auto.arima(diff(CORN[,"CORN.Close"]), stepwise = TRUE, seasonal = TRUE)

ARIMA %>% forecast() %>% autoplot()+autolayer(fitted(ARIMA))
checkresiduals(ARIMA)

SARIMA <- sarima.for(diff(CORN[,"CORN.Close"]), p = 2, d = 1, q = 1, P = 2, D = 1, Q = 1, S = 12, n.ahead = horizonte)

summary(SARIMA)

#checkresiduals(SARIMA)
  
```


Mais importante, nas séries temporais econômicas, os valores sucessivos (defasagens) tendem a estar altamente correlacionados, com o que o fantasma da multicolinearidade faz sua aparição. A multicolinearidade conduz a estimativas pouco precisas, isto é, os erros-padrão tendem a ser grandes em relação aos coeficientes estimados. Em consequência, com base nas razões $t$ estimadas, podem indicar (equivocadamente) que um coeficiente defasado é estatisticamente insignificante.

## TBATS

```{r fig.width=9, fig.height=3}

TBATS <- tbats(diff(CORN[,"CORN.Close"]))

TBATS %>% forecast() %>% autoplot()+autolayer(fitted(TBATS))
checkresiduals(TBATS)

```

## NNETAR

```{r fig.width=9, fig.height=3}

NNETAR <- nnetar(log(CORN[,"CORN.Close"]), lambda=0)

autoplot(forecast(NNETAR,h=horizonte))+autolayer(fitted(NNETAR))
#checkresiduals(NNETAR)

```

# Comparativo

```{r}

accuracy(SNAIVE)

accuracy(SES)

accuracy(HOLT)

accuracy(HOLT_aditivo)

accuracy(ARIMA)

accuracy(TBATS)

accuracy(NNETAR)

```


# Modelos de Heterocedasticidade Condicial

Assim como o termo de erro $u$ no tempo $t$ pode estar correlacionado com o termo de erro no tempo ($t – 1$) em um processo AR(1) ou com os vários termos de erro defasados em um processo geral AR($p$), pode haver autocorrelação na variância $\sigma^2$ no tempo $t$ com seus valores defasados em um ou mais períodos? 

Tal autocorrelação foi observada pelos pesquisadores na previsão de séries temporais, como preços de ações, taxas de inflação e taxas de câmbio. Essa autocorrelação recebe nomes como heterocedasticidade condicional autorregressiva (ARCH, do inglês autoregressive conditional heteroscedasticity), se a variância do erro estiver relacionada com o termo de erro elevado ao quadrado no período anterior, e heterocedasticidade condicional autorregressiva generalizada (GARCH, do inglês generalized autoregressive conditional heteroscedasticity), se a variância do erro estiver relacionada com os termos de erro elevados ao quadrado em vários períodos anteriores.

***














$$\\[1in]$$










$$\\[1in]$$


***

# Referências

Diebold, Francis X. **Elements of forecasting.** 2. ed. South Western Publishing, 2001. p. 254.

Franses, P.H. **Time Series Models for Business and Economic Forecasting.** Cambridge: Cambridge University Press, 1998.

R Core Team (2020). **R: A language and environment for statistical computing.** R Foundation for Statistical Computing, Vienna, Austria.
URL https://www.R-project.org/. 

Granger, C. W. J. **Investigating causal relations by econometric models and cross-spectral methods**. _In_ Econometrica, p. 424-438, jul. 1969.

Gujarati, D. N., Porter, D. **Econometria Básica**, quinta ed. São Paulo, 2011.

Hyndman, R., J., Athanasopoulos, G. **Forecasting:Principles and Practice**, Monash University, Australia, 2018.

Hull, J.C. **Options, Futures & Other Derivatives,** 5Th ed, 2002, Prentice Hall

Koop, G. **Analysis of economic data.** Nova York: John Wiley & Sons, 2000.

Morettin, P. **Econometria Financeira: Um curso de séries temporais financeiras**, 2a. ed. revista e ampliada, São Paulo, 2006.

Shleifer, A. **Inefficient Markets: an introduction to Behavioral Finance.** Oxford University Press, 2000.

Thurman W.N. & Fisher M.E, **Chickens, Eggs, and Causality, or Which Came First?**, _American Journal of Agricultural Economics_, 1988, 237-238.

 



